{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ul0LTa0wtKjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "s3ec6wIwBgM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 파인튜닝"
      ],
      "metadata": {
        "id": "7C8zeUVN-Ck7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 모델 경로 설정 (Google Drive 내부 경로로 변경)\n",
        "model_path = \"/content/drive/MyDrive/cv project/llama-3.2-Korean-Bllossom-3B\"\n",
        "\n",
        "# GPU 디바이스 확인\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,  # GPU에서 효율적 연산\n",
        "    device_map=None  # 직접 디바이스 지정\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# **여기에 pad_token 설정 추가**\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# JSON 파일 로드 함수\n",
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# JSON 파일 저장 함수\n",
        "def save_json(file_path, data):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# 필수 키 확인 및 PROMPT 생성 함수\n",
        "def create_prompt_and_instruction(json_data):\n",
        "    # 기본 프롬프트 정의\n",
        "    category_prompts = {\n",
        "        \"차트\": (\n",
        "            '너는 차트를 분석하여 명확하고 객관적인 요약문을 생성하는 AI이다.'\n",
        "        ),\n",
        "        \"표\": (\n",
        "            '너는 표를 분석하여 명확하고 객관적인 요약문을 생성하는 AI이다.'\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # 유형 키에 '표'가 포함되어 있는지 확인\n",
        "    if \"표\" in json_data[\"유형\"]:\n",
        "        prompt = category_prompts[\"표\"]\n",
        "    else:\n",
        "        prompt = category_prompts[\"차트\"]\n",
        "\n",
        "    # Instruction 생성\n",
        "    instruction = (\n",
        "        f\"내용: {json_data['내용']}\\n\"\n",
        "        \"위 내용을 기반으로 요약문을 작성해줘.\"\n",
        "    )\n",
        "    return prompt, instruction\n",
        "\n",
        "# 학습 데이터 로드\n",
        "json_file_path = \"/content/drive/MyDrive/cv project/기본표/학습데이터/result_table_with_summary.json\"\n",
        "json_file = load_json(json_file_path)\n",
        "\n",
        "# 데이터 준비\n",
        "data_list = []\n",
        "\n",
        "for index, json_data in enumerate(json_file):\n",
        "    # 필수 키 확인\n",
        "    required_keys = [\"data_id\", \"제목\", \"유형\", \"내용\", \"요약\"]\n",
        "    if not all(key in json_data for key in required_keys):\n",
        "        print(f\"[Error] JSON 항목 {index}가 필수 키 {required_keys}를 포함하지 않습니다.\")\n",
        "        continue\n",
        "\n",
        "    # PROMPT 및 instruction 생성\n",
        "    PROMPT, instruction = create_prompt_and_instruction(json_data)\n",
        "\n",
        "    # 입력 텍스트와 대상 텍스트 준비\n",
        "    input_text = f\"{PROMPT}\\n{instruction}\\n\"\n",
        "    target_text = json_data[\"요약\"]\n",
        "\n",
        "    # 데이터 리스트에 추가\n",
        "    data_list.append({\n",
        "        'input_text': input_text,\n",
        "        'target_text': target_text\n",
        "    })\n",
        "\n",
        "# Dataset 생성\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list(data_list)\n",
        "\n",
        "# 데이터 전처리 함수 정의\n",
        "def preprocess_function(examples):\n",
        "    input_texts = examples['input_text']\n",
        "    target_texts = examples['target_text']\n",
        "    full_texts = [input_texts[i] + target_texts[i] for i in range(len(input_texts))]\n",
        "\n",
        "    # 토크나이저 설정: padding=True로 변경\n",
        "    tokenized_inputs = tokenizer(\n",
        "        input_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_full = tokenizer(\n",
        "        full_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = tokenized_full[\"input_ids\"].clone()\n",
        "\n",
        "    # 입력 부분은 -100으로 마스킹\n",
        "    for i in range(len(input_texts)):\n",
        "        input_len = tokenized_inputs['input_ids'].shape[1]\n",
        "        labels[i, :input_len] = -100\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# 데이터셋 전처리\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 모델에 LoRA 적용\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# TrainingArguments 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/cv project/finetuningllama\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"no\"\n",
        ")\n",
        "\n",
        "# DataCollatorWithPadding 사용\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
        "\n",
        "# Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "trainer.train()\n",
        "\n",
        "# 파인튜닝된 모델 저장\n",
        "trainer.save_model(\"/content/drive/MyDrive/cv project/finetuningllama\")\n",
        "\n",
        "print(\"파인튜닝된 모델이 '/content/drive/MyDrive/cv project/finetuningllama'에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "id": "GWzIqgknhy5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# 모델 경로 설정\n",
        "original_model_path = \"/content/drive/MyDrive/cv project/llama-3.2-Korean-Bllossom-3B\"\n",
        "finetuned_model_path = \"/content/drive/MyDrive/cv project/finetuningllama\"\n",
        "\n",
        "# GPU 디바이스 확인\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 토크나이저를 원본 모델 경로에서 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad_token 설정\n",
        "\n",
        "# 모델을 원본 모델 경로에서 로드\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    original_model_path,\n",
        "    torch_dtype=torch.bfloat16,  # GPU에서 효율적 연산\n",
        "    device_map=None  # 직접 디바이스 지정\n",
        ")\n",
        "model = model.to(device)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 파인튜닝된 모델 로드 (LoRA 가중치 적용)\n",
        "model = PeftModel.from_pretrained(model, finetuned_model_path)\n",
        "\n",
        "# JSON 파일 로드 함수\n",
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# JSON 파일 저장 함수\n",
        "def save_json(file_path, data):\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Google Drive 내 JSON 파일 경로\n",
        "json_file_path = \"/content/drive/MyDrive/cv project/기본표/result_figure.json\"  # 입력 JSON 파일 경로\n",
        "output_file_path = \"/content/drive/MyDrive/cv project/기본표/result_figure_with_summary_finetuning.json\"  # 저장 경로\n",
        "\n",
        "# JSON 데이터 로드\n",
        "json_file = load_json(json_file_path)\n",
        "\n",
        "# 필수 키 확인 및 PROMPT 생성 함수 (이전 코드와 동일)\n",
        "def create_prompt_and_instruction(json_data):\n",
        "    # 기본 프롬프트 정의\n",
        "    category_prompts = {\n",
        "        \"차트\": (\n",
        "            '너는 차트를 분석하여 명확하고 객관적인 요약문을 생성하는 AI이다. 아래의 차트 정보를 정리해서 하나의 문장으로 요약문을 생성하라. 차근차근 생각해보자.'\n",
        "            '데이터에서 패턴, 공통점, 차이점, 이상치나 중요한 점이 있다면 이를 포함하라.'\n",
        "        ),\n",
        "        \"표\": (\n",
        "            '너는 표를 분석하여 명확하고 객관적인 요약문을 생성하는 AI이다. 아래의 표 정보를 정리해서 하나의 문장으로 요약문을 생성하라. 차근차근 생각해보자.'\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # 유형 키에 '표'가 포함되어 있는지 확인\n",
        "    if \"표\" in json_data[\"유형\"]:\n",
        "        prompt = category_prompts[\"표\"]\n",
        "    else:\n",
        "        prompt = category_prompts[\"차트\"]\n",
        "\n",
        "    # Instruction 생성\n",
        "    instruction = (\n",
        "        f\"다음은 {json_data['제목']}에 대한 설명입니다.\\n\"\n",
        "        f\"유형: {json_data['유형']}\\n\"\n",
        "        f\"내용: {json_data['내용']}\\n\"\n",
        "        \"위 내용을 기반으로 요약문을 작성해줘.\"\n",
        "    )\n",
        "    return prompt, instruction\n",
        "\n",
        "# JSON 파일의 모든 항목에 대해 실행\n",
        "for index, json_data in enumerate(json_file):\n",
        "    # 필수 키 확인\n",
        "    required_keys = [\"data_id\", \"제목\", \"유형\", \"내용\"]\n",
        "    if not all(key in json_data for key in required_keys):\n",
        "        print(f\"[Error] JSON 항목 {index}가 필수 키 {required_keys}를 포함하지 않습니다.\")\n",
        "        continue\n",
        "\n",
        "    # PROMPT 및 instruction 생성\n",
        "    PROMPT, instruction = create_prompt_and_instruction(json_data)\n",
        "\n",
        "    # 입력 텍스트 생성\n",
        "    input_text = f\"{PROMPT}\\n{instruction}\\n\"\n",
        "\n",
        "    # 토크나이즈 및 텐서 변환\n",
        "    input_ids = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    # 텍스트 생성\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids['input_ids'],\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # 결과 텍스트 생성\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # 입력 텍스트 부분을 제거하여 생성된 요약만 추출\n",
        "    generated_summary = generated_text[len(input_text):].strip()\n",
        "\n",
        "    print(f\"[Result for JSON {index}]\")\n",
        "    print(generated_summary)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # JSON 데이터에 요약 추가\n",
        "    json_data[\"요약\"] = generated_summary\n",
        "\n",
        "# 업데이트된 JSON 파일 저장\n",
        "save_json(output_file_path, json_file)\n",
        "\n",
        "print(f\"업데이트된 JSON 파일이 {output_file_path}에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "id": "C3qIe1bE8jEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}