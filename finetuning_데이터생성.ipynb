{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VchD18i9hAsz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ftdlW_hO-O"
      },
      "outputs": [],
      "source": [
        "# Hugging Face 라이브러리 설치\n",
        "!pip install huggingface_hub ultralytics\n",
        "!pip install doclayout-yolo\n",
        "!git clone https://github.com/opendatalab/DocLayout-YOLO.git\n",
        "# 필요한 라이브러리 설치\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install torch transformers==4.40.0 accelerate\n",
        "!pip install pytesseract\n",
        "!pip install opencv-python\n",
        "!pip install pillow\n",
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA0S0vOmhd2g"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import json\n",
        "from doclayout_yolo import YOLOv10\n",
        "import os\n",
        "import uuid\n",
        "from pdf2image import convert_from_path\n",
        "import easyocr\n",
        "from collections import Counter\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5koHco42h0UL"
      },
      "outputs": [],
      "source": [
        "class TableCellExtractor:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the TableCellExtractor with necessary components\n",
        "        \"\"\"\n",
        "        # OCR reader initialization\n",
        "        self.reader = easyocr.Reader(['ko', 'en'])\n",
        "\n",
        "    def process_image(self, image):\n",
        "      \"\"\"\n",
        "      Process the input image and extract table cells\n",
        "\n",
        "      Args:\n",
        "          image: JPG image object (numpy array or file path)\n",
        "\n",
        "      Returns:\n",
        "          dict: JSON formatted extraction results\n",
        "      \"\"\"\n",
        "      if isinstance(image, str):\n",
        "          self.image = cv2.imread(image)\n",
        "      else:\n",
        "          self.image = image\n",
        "\n",
        "      self.result = self.image.copy()\n",
        "\n",
        "      self.detect_lines()\n",
        "      self.classify_lines_and_find_intersections()\n",
        "      self.remove_duplicate_points()\n",
        "\n",
        "      # 텍스트 추출 및 셀 정보 얻기\n",
        "      data, extracted_cells = self.extract_text_from_cells()\n",
        "\n",
        "      # 데이터프레임 생성 및 처리\n",
        "      df = pd.DataFrame(data)\n",
        "\n",
        "      # 빈 행/열 제거를 위한 전처리\n",
        "      # 모든 빈 문자열을 NaN으로 변환\n",
        "      df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "      df = df.replace('', np.nan)\n",
        "\n",
        "      # 모든 값이 NaN인 행과 열 제거\n",
        "      df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
        "\n",
        "      # 인덱스 리셋\n",
        "      df = df.reset_index(drop=True)\n",
        "\n",
        "      # NaN을 다시 빈 문자열로 변환\n",
        "      df = df.fillna('')\n",
        "\n",
        "      # 처리된 데이터프레임을 기반으로 셀 정보 업데이트\n",
        "      processed_cells = []\n",
        "      for i in range(len(df)):\n",
        "          for j in range(len(df.columns)):\n",
        "              # 원본 좌표 찾기\n",
        "              original_cell = next(\n",
        "                  (cell for cell in extracted_cells\n",
        "                  if cell['row'] == i + 1 and cell['col'] == j + 1),\n",
        "                  None\n",
        "              )\n",
        "\n",
        "              if original_cell:\n",
        "                  processed_cells.append({\n",
        "                      'row': i + 1,\n",
        "                      'col': j + 1,\n",
        "                      'text': df.iloc[i, j],\n",
        "                      'coordinates': original_cell['coordinates']\n",
        "                  })\n",
        "\n",
        "      # 최종 결과를 JSON 형태로 반환\n",
        "      final_result = {\n",
        "          'cells': processed_cells,\n",
        "          'grid_info': {\n",
        "              'rows': len(df),\n",
        "              'cols': len(df.columns)\n",
        "          }\n",
        "      }\n",
        "\n",
        "      return final_result\n",
        "\n",
        "    def detect_lines(self):\n",
        "        \"\"\"\n",
        "        Detect lines in the image using Canny edge detection and Hough transform\n",
        "        \"\"\"\n",
        "        # 1. 엣지 검출 (Canny)\n",
        "        self.edges = cv2.Canny(self.image, 50, 150, apertureSize=3)\n",
        "\n",
        "        # 2. Hough 변환 적용하여 선 감지\n",
        "        self.lines = cv2.HoughLinesP(\n",
        "            self.edges,\n",
        "            1,\n",
        "            np.pi/180,\n",
        "            threshold=100,\n",
        "            minLineLength=100,\n",
        "            maxLineGap=10\n",
        "        )\n",
        "\n",
        "        return self.lines\n",
        "\n",
        "    def classify_lines_and_find_intersections(self):\n",
        "        \"\"\"\n",
        "        Classify lines as horizontal or vertical and find their intersection points\n",
        "        \"\"\"\n",
        "        self.intersection_points = []\n",
        "        self.horizontal_lines = []\n",
        "        self.vertical_lines = []\n",
        "\n",
        "        if self.lines is not None:\n",
        "            # 선 분류 (수평/수직)\n",
        "            for line in self.lines:\n",
        "                x1, y1, x2, y2 = line[0]\n",
        "                angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180.0 / np.pi)\n",
        "\n",
        "                if angle < 10 or angle > 170:\n",
        "                    self.horizontal_lines.append(line[0])\n",
        "                elif 80 < angle < 100:\n",
        "                    self.vertical_lines.append(line[0])\n",
        "\n",
        "            # 이미지의 경계에 가상의 테두리 선 추가\n",
        "            height, width = self.image.shape[:2]\n",
        "            margin = 10\n",
        "            self.horizontal_lines.append([margin, margin, width - margin, margin])  # 상단 경계선\n",
        "            self.horizontal_lines.append([margin, height - margin, width - margin, height - margin])  # 하단 경계선\n",
        "            self.vertical_lines.append([margin, margin, margin, height - margin])  # 왼쪽 경계선\n",
        "            self.vertical_lines.append([width - margin, margin, width - margin, height - margin])  # 오른쪽 경계선\n",
        "\n",
        "            # 교차점 찾기\n",
        "            self._find_intersection_points()\n",
        "\n",
        "            # 끝점 처리\n",
        "            self._process_end_points()\n",
        "\n",
        "    def _find_intersection_points(self):\n",
        "        \"\"\"\n",
        "        Calculate intersection points between horizontal and vertical lines\n",
        "        \"\"\"\n",
        "        for h_line in self.horizontal_lines:\n",
        "            for v_line in self.vertical_lines:\n",
        "                x1, y1, x2, y2 = h_line\n",
        "                x3, y3, x4, y4 = v_line\n",
        "\n",
        "                denominator = ((x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))\n",
        "                if denominator != 0:\n",
        "                    t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
        "                    u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
        "\n",
        "                    if 0 <= t <= 1 and 0 <= u <= 1:\n",
        "                        x = int(x1 + t * (x2 - x1))\n",
        "                        y = int(y1 + t * (y2 - y1))\n",
        "                        self.intersection_points.append((x, y))\n",
        "        # 교차점 정렬\n",
        "        self.intersection_points = sorted(set(self.intersection_points), key=lambda p: (p[1], p[0]))\n",
        "\n",
        "    def _process_end_points(self):\n",
        "        \"\"\"\n",
        "        Process end points of lines and combine with intersection points\n",
        "        \"\"\"\n",
        "        # 끝점 수집\n",
        "        end_points = []\n",
        "        for line in self.horizontal_lines + self.vertical_lines:\n",
        "            x1, y1, x2, y2 = line\n",
        "            end_points.append((x1, y1))\n",
        "            end_points.append((x2, y2))\n",
        "\n",
        "        # 최소/최대 좌표 계산\n",
        "        x_values = [point[0] for point in end_points]\n",
        "        y_values = [point[1] for point in end_points]\n",
        "\n",
        "        x_min, x_max = min(x_values), max(x_values)\n",
        "        y_min, y_max = min(y_values), max(y_values)\n",
        "\n",
        "        # 필터링된 끝점 선택\n",
        "        self.filtered_end_points = [\n",
        "            (x, y) for (x, y) in end_points\n",
        "            if (x_min <= x <= x_min + 10 or x_max - 10 <= x <= x_max) or\n",
        "               (y_min <= y <= y_min + 10 or y_max - 10 <= y <= y_max)\n",
        "        ]\n",
        "\n",
        "        # 모든 점 결합\n",
        "        self.all_points = self.intersection_points + self.filtered_end_points\n",
        "\n",
        "    def remove_duplicate_points(self, distance_threshold=15):\n",
        "        \"\"\"\n",
        "        Remove duplicate points that are within a certain distance threshold\n",
        "\n",
        "        Args:\n",
        "            distance_threshold (int): Maximum distance between points to be considered duplicates\n",
        "        \"\"\"\n",
        "        self.unique_points = []\n",
        "        points_array = np.array(self.all_points)\n",
        "\n",
        "        # 각 점에 대해 거리를 계산하여 중복 제거\n",
        "        for point in self.all_points:\n",
        "            is_unique = True\n",
        "            for unique_point in self.unique_points:\n",
        "                distance = np.linalg.norm(np.array(point) - np.array(unique_point))\n",
        "                if distance <= distance_threshold:\n",
        "                    is_unique = False\n",
        "                    break\n",
        "            if is_unique:\n",
        "                self.unique_points.append(point)\n",
        "\n",
        "    def extract_text_from_cells(self, min_height=30, min_width=30):\n",
        "      \"\"\"\n",
        "      Extract text from each cell in the table grid and return with cell coordinates\n",
        "\n",
        "      Args:\n",
        "          min_height (int): Minimum height of cell to process\n",
        "          min_width (int): Minimum width of cell to process\n",
        "\n",
        "      Returns:\n",
        "          tuple: (2D list of extracted text, list of cell information)\n",
        "      \"\"\"\n",
        "      # x, y 좌표 분리 및 정렬\n",
        "      self.x_coords = sorted(list(set([point[0] for point in self.intersection_points])))\n",
        "      self.y_coords = sorted(list(set([point[1] for point in self.intersection_points])))\n",
        "\n",
        "      # 격자 구간별 텍스트 추출\n",
        "      data = []\n",
        "      extracted_cells = []\n",
        "\n",
        "      for i in range(len(self.y_coords) - 1):\n",
        "          row = []\n",
        "          for j in range(len(self.x_coords) - 1):\n",
        "              # 격자 영역 좌표 계산\n",
        "              top_left_x = self.x_coords[j]\n",
        "              top_left_y = self.y_coords[i]\n",
        "              bottom_right_x = self.x_coords[j + 1]\n",
        "              bottom_right_y = self.y_coords[i + 1]\n",
        "\n",
        "              # 격자 영역 잘라내기\n",
        "              tile = self.image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
        "\n",
        "              # 셀 정보 생성\n",
        "              cell_info = {\n",
        "                  'row': i + 1,\n",
        "                  'col': j + 1,\n",
        "                  'coordinates': {\n",
        "                      'top_left': (top_left_x, top_left_y),\n",
        "                      'bottom_right': (bottom_right_x, bottom_right_y)\n",
        "                  }\n",
        "              }\n",
        "\n",
        "              # 너무 작은 이미지는 빈 텍스트로 처리\n",
        "              if tile.shape[0] < min_height or tile.shape[1] < min_width:\n",
        "                  row.append(\"\")\n",
        "                  cell_info['text'] = \"\"\n",
        "                  extracted_cells.append(cell_info)\n",
        "                  continue\n",
        "\n",
        "              # EasyOCR로 텍스트 추출\n",
        "              text_result = self.reader.readtext(tile, detail=0)\n",
        "              text = \"\\n\".join(text_result).strip()\n",
        "              row.append(text)\n",
        "              cell_info['text'] = text\n",
        "              extracted_cells.append(cell_info)\n",
        "\n",
        "          data.append(row)\n",
        "\n",
        "      return data, extracted_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S664FWqth3mP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import easyocr\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_text_from_cells(cells_data):\n",
        "    \"\"\"Extract text content from cells data\"\"\"\n",
        "    extracted_text = []\n",
        "    for cell in cells_data:\n",
        "        if 'text' in cell:\n",
        "            extracted_text.append(cell['text'])\n",
        "    return ' '.join(extracted_text)\n",
        "\n",
        "def process_images_in_directory(input_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Process all images in a directory and extract text.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Directory containing the images.\n",
        "        output_dir (str): Directory to save the JSON file with extracted text.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_dir):\n",
        "        raise ValueError(f\"Input directory does not exist: {input_dir}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Initialize TableCellExtractor once to reuse the EasyOCR reader\n",
        "    processor = TableCellExtractor()\n",
        "\n",
        "    # Iterate through all images in the directory\n",
        "    for file_name in tqdm(os.listdir(input_dir), desc=\"Processing Images\"):\n",
        "        if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")):\n",
        "            image_path = os.path.join(input_dir, file_name)\n",
        "\n",
        "            # Read image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Failed to read image: {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Process image and extract text\n",
        "            try:\n",
        "                # Process the image using TableCellExtractor\n",
        "                result = processor.process_image(image)\n",
        "\n",
        "                if result and 'cells' in result and 'grid_info' in result:\n",
        "                    # Extract text from cells\n",
        "                    extracted_text = extract_text_from_cells(result['cells'])\n",
        "\n",
        "                    json_data = {\n",
        "                        \"data_id\": file_name,\n",
        "                        \"제목\": file_name,  # 임시로 파일명 사용\n",
        "                        \"유형\": \"표\",\n",
        "                        \"내용\": extracted_text,\n",
        "                    }\n",
        "                    results.append(json_data)\n",
        "                else:\n",
        "                    print(f\"No valid table data found in {file_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "    # Save results as JSON\n",
        "    save_results_as_json(results, output_dir)\n",
        "\n",
        "def save_results_as_json(results, output_dir):\n",
        "    \"\"\"Save results to a JSON file.\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # JSON save path\n",
        "    results_json_path = os.path.join(output_dir, \"result_table.json\")\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(results_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
        "    print(f\"Extracted text saved to: {results_json_path}\")\n",
        "\n",
        "def process_single_result(result, file_name):\n",
        "    \"\"\"\n",
        "    Process a single result dictionary and convert to desired format.\n",
        "\n",
        "    Args:\n",
        "        result (dict): Dictionary containing cells and grid_info\n",
        "        file_name (str): Name of the file being processed\n",
        "    \"\"\"\n",
        "    extracted_text = extract_text_from_cells(result['cells'])\n",
        "\n",
        "    json_data = {\n",
        "        \"data_id\": file_name,\n",
        "        \"제목\": file_name,\n",
        "        \"유형\": \"표\",\n",
        "        \"내용\": extracted_text,\n",
        "    }\n",
        "\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cLgfsntAh5_d"
      },
      "outputs": [],
      "source": [
        "# 여러 이미지 처리할 경우\n",
        "input_dir = \"/content/drive/MyDrive/cv project/기본표/기본표jpg\"\n",
        "output_dir = \"/content/drive/MyDrive/cv project/기본표/학습데이터\"\n",
        "process_images_in_directory(input_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daSp1F7FnTSi"
      },
      "outputs": [],
      "source": [
        "# Load the JSON data from the file\n",
        "file_path = '/content/outputs/result_table.json'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Process the JSON to remove the content of the \"제목\" key and completely remove the \"cells\" key\n",
        "for entry in data:\n",
        "    if \"제목\" in entry:\n",
        "        entry[\"제목\"] = \"\"\n",
        "    if \"cells\" in entry:\n",
        "        del entry[\"cells\"]\n",
        "    if \"grid\" in entry:\n",
        "        del entry[\"grid\"]\n",
        "    entry[\"요약\"] = \"\"  # Add an empty \"요약\" key\n",
        "\n",
        "# Save the modified JSON back to a file\n",
        "output_file_path = '/content/outputs/result_table_modified.json'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQVx5k97pkE7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Paths to the necessary files and folders\n",
        "result_table_path = '/content/drive/MyDrive/cv project/기본표/학습데이터/result_table.json'\n",
        "folder_path = '/content/drive/MyDrive/cv project/기본표/기본표json'\n",
        "output_path = '/content/drive/MyDrive/cv project/기본표/학습데이터/result_table_with_summary.json'\n",
        "\n",
        "try:\n",
        "    # Load the result_table.json\n",
        "    with open(result_table_path, 'r', encoding='utf-8') as result_file:\n",
        "        result_table = json.load(result_file)\n",
        "\n",
        "    # Iterate over each entry in result_table\n",
        "    for entry in result_table:\n",
        "        data_id = entry.get(\"data_id\", \"\")\n",
        "        if not data_id:\n",
        "            continue\n",
        "\n",
        "        # Extract the matching file name (remove .jpg)\n",
        "        base_name = data_id.rsplit(\".\", 1)[0]\n",
        "        target_json_path = os.path.join(folder_path, f\"{base_name}.json\")\n",
        "\n",
        "        # Check if the corresponding JSON file exists\n",
        "        if os.path.exists(target_json_path):\n",
        "            # Load the corresponding JSON file\n",
        "            with open(target_json_path, 'r', encoding='utf-8') as json_file:\n",
        "                target_data = json.load(json_file)\n",
        "\n",
        "            # Extract the value of \"table_data.text_explanation\"\n",
        "            table_data = target_data.get(\"table_data\", {})\n",
        "            text_explanation = table_data.get(\"table_data.text_explanation\", \"\")\n",
        "\n",
        "            # Add the text_explanation value to the 요약 key in the current entry\n",
        "            entry[\"요약\"] = text_explanation\n",
        "        else:\n",
        "            # If the file does not exist, set 요약 as an empty string\n",
        "            entry[\"요약\"] = \"\"\n",
        "\n",
        "    # Save the updated result_table.json with the 요약 key added\n",
        "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "        json.dump(result_table, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Updated result_table.json has been saved to: {output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}